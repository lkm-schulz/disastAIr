{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set ~~Shenanigans~~ Labeling\n",
    "\n",
    "The kaggle competition that serves as the base for our project provides an unlabeled test set (which makes a lot of sense in view of the competition).\n",
    "For us, however, a labeled test set would allow for a more detailed analysis of the results (e.g., by plotting a confusion matrix).\n",
    "\n",
    "Luckily, some (hours of) digging around the internet brought up a [dataset](https://github.com/shrnik/Disater_Pred/blob/master/socialmedia-disaster-tweets-DFE.csv) which seems to be the origin of the data used for this competition.\n",
    "This notebook takes the unlabeled competition test set and extends it with the labels found in the original data set for the corresponding entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "LABEL_TRUE = 'Relevant'\n",
    "LABEL_FALSE = 'Not Relevant'\n",
    "\n",
    "DIR_DATA = os.path.join(\"..\", \"data\")\n",
    "PATH_DATA_TEST = os.path.join(DIR_DATA, \"test.csv\")\n",
    "PATH_DATA_FULL = os.path.join(DIR_DATA, \"full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Interestingly enough, the full dataset has some non-Unicode encoded characters. \n",
    "For those to be interpreted like they are in the kaggle test set the file has to be read with the `latin-1` encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe 'Kaggle Test Set' has:\n",
      "\t3263 rows\n",
      "\twith columns: ['id', 'keyword', 'location', 'text']\n",
      "\n",
      "Dataframe 'Full Data Set' has:\n",
      "\t10876 rows\n",
      "\twith columns: ['_unit_id', '_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at', 'choose_one', 'choose_one:confidence', 'choose_one_gold', 'keyword', 'location', 'text', 'tweetid', 'userid']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_info(name, df: pd.DataFrame):\n",
    "    print(f'Dataframe \\'{name}\\' has:')\n",
    "    print(f'\\t{len(df)} rows')\n",
    "    print(f'\\twith columns: {df.columns.to_list()}')\n",
    "    print()\n",
    "\n",
    "df_test = pd.read_csv(PATH_DATA_TEST)\n",
    "df_full = pd.read_csv(PATH_DATA_FULL, encoding='latin_1')\n",
    "\n",
    "len_test = len(df_test)\n",
    "\n",
    "print_info(\"Kaggle Test Set\", df_test)\n",
    "print_info(\"Full Data Set\", df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each entry in the test set, find the corresponding entry (so the one with the same text) in the full dataset.\n",
    "Some texts appear multiple times in the full dataset, leading to duplicate entries for some rows. Those are filtered out for the `df_merged_uniques` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n",
      "Dropped 214 entries with duplicated ids\n"
     ]
    }
   ],
   "source": [
    "df_merged = pd.merge(df_test, df_full[['text', 'choose_one']], on='text', how='left')\n",
    "df_merged_uniques = df_merged.drop_duplicates(subset='id')\n",
    "\n",
    "print(len(df_merged_uniques))\n",
    "\n",
    "print(f'Dropped {len(df_merged) - len(df_merged_uniques)} entries with duplicated ids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some duplicates, the target is not the same (the same text is classified as disaster and no-disaster). \n",
    "For those instances, manual adjustments will be needed to find the labels that are expected by kaggle (and thus should be used as the labels of the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Instances with Uncertain Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 instances with uncertain targets.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>922</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>924</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>1931</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>Dublin City, Ireland</td>\n",
       "      <td>@RockBottomRadFM Is one of the challenges on T...</td>\n",
       "      <td>Can't Decide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>1964</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>? High Skies - Burning Buildings ? http://t.co...</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>3094</td>\n",
       "      <td>deaths</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bigamist and his Û÷firstÛª wife are charged ...</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>3374</td>\n",
       "      <td>demolition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General News Û¢åÊ'Demolition of houses on wat...</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>4053</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>4056</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>4371</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>in the Word of God</td>\n",
       "      <td>@GreenLacey GodsLove &amp;amp; #thankU my sister f...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>4572</td>\n",
       "      <td>emergency%20plan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you have an emergency drinking water plan? ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>4653</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>4837</td>\n",
       "      <td>evacuation</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Evacuation drill at work. The fire doors would...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>4930</td>\n",
       "      <td>exploded</td>\n",
       "      <td>Ittihad .f.c</td>\n",
       "      <td>that exploded &amp;amp; brought about the\\nbeginni...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>4949</td>\n",
       "      <td>exploded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that exploded &amp;amp; brought about the\\nbeginni...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>5679</td>\n",
       "      <td>floods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Who is bringing the tornadoes and floods. Who ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>6101</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hellfire is surrounded by desires so be carefu...</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>6122</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>???? ???????</td>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>6131</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Prophet (peace be upon him) said 'Save you...</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>6539</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLEARED:incident with injury:I-495  inner loop...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>8011</td>\n",
       "      <td>refugees</td>\n",
       "      <td>Kansas  KS</td>\n",
       "      <td>'imagine an entire aisle dedicated to making p...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>8033</td>\n",
       "      <td>refugees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292</th>\n",
       "      <td>10232</td>\n",
       "      <td>volcano</td>\n",
       "      <td>Planet Earth</td>\n",
       "      <td>Learning from the Legacy of a Catastrophic Eru...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id              keyword                  location  \\\n",
       "296     922         bioterrorism                       NaN   \n",
       "302     924         bioterrorism                       NaN   \n",
       "622    1931  burning%20buildings      Dublin City, Ireland   \n",
       "631    1964  burning%20buildings             San Francisco   \n",
       "970    3094               deaths                       NaN   \n",
       "1062   3374           demolition                       NaN   \n",
       "1292   4053            displaced  Pedophile hunting ground   \n",
       "1298   4056            displaced  Pedophile hunting ground   \n",
       "1401   4371           earthquake        in the Word of God   \n",
       "1463   4572     emergency%20plan                       NaN   \n",
       "1491   4653             engulfed                       NaN   \n",
       "1542   4837           evacuation                  Brisbane   \n",
       "1567   4930             exploded              Ittihad .f.c   \n",
       "1578   4949             exploded                       NaN   \n",
       "1808   5679               floods                       NaN   \n",
       "1937   6101             hellfire                       NaN   \n",
       "1947   6122             hellfire              ???? ???????   \n",
       "1953   6131             hellfire                       NaN   \n",
       "2090   6539               injury                       NaN   \n",
       "2559   8011             refugees                Kansas  KS   \n",
       "2564   8033             refugees                       NaN   \n",
       "3292  10232              volcano              Planet Earth   \n",
       "\n",
       "                                                   text    choose_one  \n",
       "296                          To fight bioterrorism sir.      Relevant  \n",
       "302                          To fight bioterrorism sir.      Relevant  \n",
       "622   @RockBottomRadFM Is one of the challenges on T...  Can't Decide  \n",
       "631   ? High Skies - Burning Buildings ? http://t.co...  Not Relevant  \n",
       "970   Bigamist and his Û÷firstÛª wife are charged ...  Not Relevant  \n",
       "1062  General News Û¢åÊ'Demolition of houses on wat...  Not Relevant  \n",
       "1292  .POTUS #StrategicPatience is a strategy for #G...      Relevant  \n",
       "1298  .POTUS #StrategicPatience is a strategy for #G...      Relevant  \n",
       "1401  @GreenLacey GodsLove &amp; #thankU my sister f...      Relevant  \n",
       "1463  Do you have an emergency drinking water plan? ...      Relevant  \n",
       "1491  He came to a land which was engulfed in tribal...  Not Relevant  \n",
       "1542  Evacuation drill at work. The fire doors would...      Relevant  \n",
       "1567  that exploded &amp; brought about the\\nbeginni...      Relevant  \n",
       "1578  that exploded &amp; brought about the\\nbeginni...      Relevant  \n",
       "1808  Who is bringing the tornadoes and floods. Who ...      Relevant  \n",
       "1937  Hellfire is surrounded by desires so be carefu...  Not Relevant  \n",
       "1947  #Allah describes piling up #wealth thinking it...  Not Relevant  \n",
       "1953  The Prophet (peace be upon him) said 'Save you...  Not Relevant  \n",
       "2090  CLEARED:incident with injury:I-495  inner loop...      Relevant  \n",
       "2559  'imagine an entire aisle dedicated to making p...      Relevant  \n",
       "2564  wowo--=== 12000 Nigerian refugees repatriated ...      Relevant  \n",
       "3292  Learning from the Legacy of a Catastrophic Eru...      Relevant  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_merged_dups = df_merged[df_merged.duplicated(subset='text', keep=False)]\n",
    "df_merged_dups_diff = df_merged_dups.groupby('text').filter(lambda x: x['choose_one'].nunique() > 1)\n",
    "df_merged_dups_diff_uniques = df_merged_dups_diff.drop_duplicates(subset=['text', 'id'])\n",
    "print(f'Found {len(df_merged_dups_diff_uniques)} instances with uncertain targets.')\n",
    "\n",
    "display(df_merged_dups_diff_uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is pretty much manual labor:\n",
    "- look at the entries with uncertain targets\n",
    "- think about which target they should have\n",
    "- modify their target value\n",
    "- upload the prediction to kaggle and see whether the score improves\n",
    "\n",
    "A single account is allowed to submit 5 predictions per day on kaggle, so a multitude of accounts had to be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n",
      "Changing prediction for id 924 to Not Relevant\n",
      "Changing prediction for id 1931 to Not Relevant\n",
      "Changing prediction for id 4949 to Not Relevant\n",
      "Changing prediction for id 10232 to Not Relevant\n",
      "Adjusted 4 predictions\n"
     ]
    }
   ],
   "source": [
    "adjustments = 0\n",
    "\n",
    "def change_pred(id, pred):\n",
    "    global df_merged_uniques_adjusted\n",
    "    global adjustments\n",
    "\n",
    "    if (df_merged_uniques_adjusted.loc[df_merged_uniques_adjusted['id'] == id, 'choose_one'] != pred).any():\n",
    "        adjustments += 1\n",
    "        print(f'Changing prediction for id {id} to {pred}')\n",
    "        df_merged_uniques_adjusted.loc[df_merged_uniques_adjusted['id'] == id, 'choose_one'] = pred\n",
    "\n",
    "df_merged_uniques_adjusted = df_merged_uniques.copy()\n",
    "\n",
    "print(len(df_merged_uniques_adjusted))\n",
    "\n",
    "change_pred(922, LABEL_TRUE)\n",
    "change_pred(924, LABEL_FALSE)\n",
    "\n",
    "change_pred(1931, LABEL_FALSE)  # No Change\n",
    "change_pred(1964, LABEL_FALSE)  # No Change\n",
    "\n",
    "change_pred(3094, LABEL_FALSE)  # apparently not true.. \n",
    "\n",
    "change_pred(4053, LABEL_TRUE) \n",
    "change_pred(4056, LABEL_TRUE)\n",
    "\n",
    "change_pred(4371, LABEL_TRUE)   # apparently true...\n",
    "change_pred(4572, LABEL_TRUE)  \n",
    "\n",
    "change_pred(4837, LABEL_TRUE)   # :(\n",
    "change_pred(4930, LABEL_TRUE)   # :(\n",
    "change_pred(4949, LABEL_FALSE)  # yeeee\n",
    "\n",
    "change_pred(5679, LABEL_TRUE)   # :(\n",
    "\n",
    "change_pred(8011, LABEL_TRUE)  \n",
    "change_pred(10232, LABEL_FALSE) # wooo this is actually an issue\n",
    "\n",
    "print(f'Adjusted {adjustments} predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_labeled = df_merged_uniques_adjusted.copy()\n",
    "df_test_labeled = df_test_labeled.rename(columns={'choose_one': 'target'})\n",
    "df_test_labeled['target'] = df_test_labeled['target'].apply(lambda x: 1 if x == LABEL_TRUE else 0)\n",
    "\n",
    "assert(len(df_test_labeled) == len_test)\n",
    "\n",
    "df_test_labeled.to_csv(\"../data/test-labeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df_merged_uniques_adjusted[['id']].copy()\n",
    "df_submission['target'] = df_merged_uniques_adjusted['choose_one'].apply(lambda x: 1 if x == LABEL_TRUE else 0)\n",
    "\n",
    "assert(len(df_submission) == len_test)\n",
    "\n",
    "df_submission.to_csv(\"../data/submission2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
